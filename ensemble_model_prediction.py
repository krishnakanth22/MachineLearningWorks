# -*- coding: utf-8 -*-
"""Ensemble_model_prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jRVhfVg4sTASu32F-87wSkbOSJ1oj0Fo
"""

!wget https://github.com/krishnakanth22/MachineLearningWorks/blob/main/Earthquake/Dataset/Earthquake_Data.csv -P /content/Earthquake/

import pandas as pd
from google.colab import data_table

data_table.enable_dataframe_formatter()

df = pd.read_csv("/content/Earthquake/Earthquake_Data.csv", delimiter=r'\s+')
data_table.DataTable(df)
display(df)

new_column_names = ["Date(YYYY/MM/DD)",  "Time(UTC)", "Latitude(deg)", "Longitude(deg)", "Depth(km)", "Magnitude(ergs)",
                    "Magnitude_type", "No_of_Stations", "Gap", "Close", "RMS", "SRC", "EventID"]

df.columns = new_column_names
ts = pd.to_datetime(df["Date(YYYY/MM/DD)"] + " " + df["Time(UTC)"])
df = df.drop(["Date(YYYY/MM/DD)", "Time(UTC)"], axis=1)
df.index = ts
display(df)

df.info()

import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
X = df[['Latitude(deg)', 'Longitude(deg)', 'Depth(km)', 'No_of_Stations']]
y = df['Magnitude(ergs)']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split

# Assuming you have already defined X and y
# X = df[['Latitude(deg)', 'Longitude(deg)', 'Depth(km)', 'No_of_Stations']]
# y = df['Magnitude(ergs)']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Create DataFrame for train and test data with labels
train_df = pd.concat([X_train['Depth(km)'], y_train], axis=1)
train_df['Set'] = 'Train'
test_df = pd.concat([X_test['Depth(km)'], y_test], axis=1)
test_df['Set'] = 'Test'

# Concatenate train and test data
concatenated_df = pd.concat([train_df, test_df])

# Plotting
plt.figure(figsize=(10, 6))
bar_width = 0.35
index = np.arange(len(concatenated_df))

train_bars = plt.bar(index[concatenated_df['Set'] == 'Train'], concatenated_df[concatenated_df['Set'] == 'Train']['Magnitude(ergs)'], bar_width, label='Train')
test_bars = plt.bar(index[concatenated_df['Set'] == 'Test'] + bar_width, concatenated_df[concatenated_df['Set'] == 'Test']['Magnitude(ergs)'], bar_width, label='Test')

plt.xlabel('Data Points')
plt.ylabel('Magnitude(ergs)')
plt.title('Magnitude vs Depth for Train and Test Data')
plt.xticks(index + bar_width, index)
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

fig, axs = plt.subplots(2, 1, figsize=(6, 6))

# Depth distribution
axs[0].hist(X_train['Depth(km)'], bins=20, alpha=0.5, color='yellow', label='Train Data')
axs[0].hist(X_test['Depth(km)'], bins=20, alpha=0.5, color='blue', label='Test Data')
axs[0].set_title('Depth Distribution')
axs[0].set_xlabel('Depth (km)')
axs[0].set_ylabel('Frequency')
axs[0].legend()

# Magnitude distribution
axs[1].hist(y_train, bins=20, alpha=0.5, color='orange', label='Train Data')
axs[1].hist(y_test, bins=20, alpha=0.5, color='blue', label='Test Data')
axs[1].set_title('Magnitude Distribution')
axs[1].set_xlabel('Magnitude (ergs)')
axs[1].set_ylabel('Frequency')
axs[1].legend()
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select relevant features for correlation analysis
selected_features = ['Latitude(deg)', 'Longitude(deg)', 'Depth(km)', 'No_of_Stations', 'Gap', 'Close', 'RMS']
correlation_matrix = df[selected_features + ['Magnitude(ergs)']].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

import pandas as pd
import numpy as np

selected_features = ['Latitude(deg)', 'Longitude(deg)', 'Depth(km)', 'No_of_Stations', 'Gap', 'Close', 'RMS', 'Magnitude(ergs)']
selected_df = df[selected_features]
rank_df = selected_df.rank(axis=0)
differences = rank_df.diff()
differences = differences.dropna()
squared_differences = differences ** 2
sum_squared_diff = squared_differences.sum()
n = len(selected_df)
spearman_corr_coeff = 1 - (6 * sum_squared_diff.sum()) / (n * (n**2 - 1))
print("Spearman's correlation coefficient:", spearman_corr_coeff)

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate Spearman's correlation matrix
spearman_corr_matrix = selected_df.corr(method='spearman')
# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(spearman_corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Spearman\'s Correlation Matrix')
plt.show()

from sklearn.ensemble import AdaBoostRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error

# AdaBoost
ada_model = AdaBoostRegressor()
ada_model.fit(X_train, y_train)

# XGBoost
xgb_model = XGBRegressor()
xgb_model.fit(X_train, y_train)

# LightGBM
lgb_model = LGBMRegressor()
lgb_model.fit(X_train, y_train)

# Decision Tree
dt_model = DecisionTreeRegressor()
dt_model.fit(X_train, y_train)

# Lasso regression
lasso_model = Lasso()
lasso_model.fit(X_train, y_train)

# Ensemble Stack
def ensemble_predict(models, X):
    predictions = np.column_stack([model.predict(X) for model in models])
    return np.mean(predictions, axis=1)  # Averaging predictions
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

ensemble_models = [ada_model, xgb_model, lgb_model, dt_model, lasso_model]

ensemble_predictions = ensemble_predict(ensemble_models, X_test)
ensemble_rmse = rmse(y_test, ensemble_predictions)
print("Ensemble Model RMSE:", ensemble_rmse)

from sklearn.ensemble import AdaBoostRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import Lasso
from sklearn.metrics import r2_score, mean_absolute_error
from tabulate import tabulate


def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    # R2 score MAE(evaluation)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)

    return r2, mae

ada_model = AdaBoostRegressor()
xgb_model = XGBRegressor()
lgb_model = LGBMRegressor()
dt_model = DecisionTreeRegressor()
lasso_model = Lasso()

models = {'AdaBoost': ada_model, 'XGBoost': xgb_model, 'LightGBM': lgb_model,
          'Decision Tree': dt_model, 'Lasso Regression': lasso_model}

results = []
for name, model in models.items():
    r2, mae = train_and_evaluate_model(model, X_train, y_train, X_test, y_test)
    results.append([name, r2, mae])

'''Print results
for name, scores in results.items():'''

print(tabulate(results, headers=['Model', 'R2 Score', 'MAE Score'], tablefmt='grid'))

from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_absolute_error

xgb_model = XGBRegressor(max_depth=6, learning_rate=0.2, n_estimators=300, min_child_weight=2, reg_alpha=0.005, reg_lambda=6)
xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print("R2 Score:", r2)
print("MAE Score:", mae)

from lightgbm import LGBMRegressor
from sklearn.metrics import r2_score, mean_absolute_error

lgbm_model = LGBMRegressor(max_depth=6, num_boost_round=300)  # For LightGBM 1
lgbm_model.fit(X_train, y_train)

y_pred = lgbm_model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print("R2 Score:", r2)
print("MAE Score:", mae)

